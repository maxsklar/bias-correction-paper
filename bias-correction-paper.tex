\documentclass[twoside]{article}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{pgfplots}
\usepackage{multicol}
\usepackage[hmarginratio=1:1,top=32mm,columnsep=20pt]{geometry}
\usepackage{fullpage}
\usepackage{pdflscape}
\usepackage[toc,page]{appendix}
\usepackage[shortlabels]{enumitem}
\usepackage{draftwatermark}

\begin{document}
\parindent=0in
\parskip=12pt

\SetWatermarkText{Draft}
\SetWatermarkScale{5}

\title{
  Sampling Bias Correction for Supervised Machine Learning \\
  \large{
    A Bayesian Inference Approach with Practical Applications
  }
}

\author{Max Sklar}

\maketitle
\thispagestyle{empty}

\begin{abstract}
Given a supervised machine learning problem where the training set has been subject to a known sampling bias, we still want to train a model to fit the original dataset. This can be achieved by introducing a sampling function to the Bayesian inference formula, and deriving an altered posterior distribution for the task. We then apply this to the common case of binary logistic regression, and discuss scenarios where a dataset might be subject to intentional sample bias such as label imbalance. This technique is widely applicable to statistical inference on big data, from the medical sciences to image recognition to marketing. Familiarity with it will give the reader tools to improve their inference pipeline from data collection to model selection. 
\end{abstract}

\section{Introduction}
\label{section:introduction}

Developing \textit{algorithms} is a central task in computer science. Each algorithm is a series of precise instructions to compute a given mathematical \textit{function} \(f: X \to Y\).

This works great when the programmer has these precise instructions available, but not when a function is only partially known, or if it is too complex for even a large team. \textit{Machine Learning} was developed in part as a response to this problem, to extend the ultimate reach of software. 

In \textit{supervised machine learning}, an algorithm \(learns\) to compute a specific function \(f\) through example rather than through direct instructions. Examples are given in the form of \(instances\) of input-output pairs \((x, y) \in X \times Y\) known as the \textit{training set}. The training set allows the machine to learn the \textit{concept} of \(f\) by producing an approximation of it known as a \textit{model}. Models are often evaluated on a \textit{test set} of instances and finally deployed on real world data to make predictions.

Machine learning is most reliable when the training set approximates these real world conditions. This isn't always possible. Models might perform well anyway, but that assumption is far from certain to hold.

Occasionally an accurate training set is unavailable either by design or circumstance, and all that remains is a biased sample. This could happen in many ways. Databases that contain vast market knoweldge could have had records deleted in a non-random way. Usually datasets and experiments involving humans have sensitive information that will need to be redacted and filtered. Finally - and perhaps most instructively - vast amounts of data might be deemed unhelpful to solving the problem in question, and it could be temporarily discarded in order to save costs.

Fortunately, there is a significant subset of these cases for which this bias is not a problem: \textbf{When the sampling function is known, there is a method that can learn the concept from the remaining evidence available, and produce a model to fit the \(original\) dataset.}

In this paper, we will fully document this method. We analyze the problem through the \textit{Bayesian framework} for supervised machine learning reviewed in section \ref{framework}. Key to this framework is the \textit{posterior probability distribution} over potential models, which allows a search algorithm to discriminate between them by their projected performance. When a bias sample is provided, the posterior distribution formula can be properly corrected as detailed in sections \ref{section:problem} and \ref{section:solution} to counteract this bias.

Unlike ad-hoc methods for correcting bias after a model has been created, this method bakes the bias-counteracting term into the learning process itself and is shown here to be consistent with first principles. It also has been successfully deployed in major commerical product launches, as documented in section \ref{section:visit}.

\section{Motivation}

While big data has unlocked incredible new applications of machine learning, the accompanied processing comes at a heavy cost in terms of time, money, and energy. Many algorithms will perform just as well or almost as well had they been trained on a fraction of the available data. Therefore, removing data from the training set is a legitimate design decision.

One way to slim down the training set is through \textit{uniform random sampling}, where each instance in the original dataset has an equal probability of inclusion. This process ignores the fact that some training examples are more valuable than others, particularly when there is a classification imbalance.

\subsection{Example: Theoretical Image Recognition}
\label{section:lion}

Suppose that the goal is to train an image recognition algorithm to detect lions. The training data contains 2000 images of lions, and 400,000 images that are \textbf{not} lions. Consider the following 3 sampling schemes.

\begin{enumerate}[label=(\Alph*)]
\item Train on all of the data (2000 Lions, 400000 Non-Lions)
\item Randomly sample 25 percent of the data (500 Lions, 100000 Non-Lions)
\item Randomly sample a quarter of ONLY the non-lion images (2000 Lions, 100000 Non-Lions)
\end{enumerate}

Each option comes with tradeoffs.

Option A uses all of the available data and takes the most time and resources to train. In theory, this will produce a model that is at least as good as any of the others. In practice this might not be the case because the search algorithm is imperfect. For example, if a \textit{mini-batch} system is used to find gradients and search for the best model, it might not converge as easily with an imbalanced dataset.

Option B has all the imbalance of A, but with fewer instances. The learning algoithm will run faster, but will likely product worse results. Any classifier must rely on lion photos to learn lions, and those have become quite scarce! A reduction in lion photos from 2000 to 500 will make a significant negative impact. Because of the imbalance, each lion photo is much more valuable to our success than a non-lion photo. 

Option C, like option B, demands fewer resources. The difference now is that all 2000 lion photos remain. The likely outcome is that option C performs far better than option B. It might even perform as well as option A! If this is the case, then C is the best choice.

Because option C only removed non-lion images, it will learn that lions are more common than they really are in the underlying dataset. Our sampling method and therefore our derived dataset now have a \textit{bias} towards lions over non-lions. This causes it to overpredict the lions, particularly in those marginal cases where it is uncertain.

Hopefully the underlying image recognition algorithm will derive the core visual features of a lion and will not have that issue even without need for correction, but such a correction is desirable.

\subsection{Example: Probabilistic Event Detection}
\label{section:visit}

In the image recognition example, bias correction is desirable. In other cases, it is crucial!

An iteration of Foursquare’s attribution model\cite{visitprediction} provides a clear example. The company's attribution product measures the ability of an advertisement to drive consumers to physical locations such as a retail chain. In order to estimate causality, the Foursquare data pipeline must first learn the base probability that any given individual will visit that chain on a given day.

Foursquare's data set has many examples of visits, but examples of people who “did not visit” on any given day outnumbers visit by several orders of magnitude. Therefore, it makes sense to downsample those non-visits. Because ad measurement products must produce precise probabilities, that sampling must be accounted for appropriately. This was ultimately achieved through the bias correction formula for logistic regression in section \ref{section:logistic}.

\subsection{Similar and Adjacent Work}

The literature on sampling and rare events is vast. Rather than providing a comprehensive review, we refer to some selected works which were either helpful or inspirational in the course of this research.

For general cataloging of situations with imbalanced classification and techniques for it's mitigation, see the work of Maalouf and Trafalis\cite{rareevents}. The readjustment formulas in the case of logistic regression can be found in Maalouf and Siddiqi\cite{weightedlogistic}. For an in-depth discussion on rare events in logistic regression, the problems associated with it, and the mathematics of parameter estimation, see King and Zeng\cite{king}.

\section{Supervised Machine Learning} \label{framework}

The following is a typical setup for supervised machine learning using a Bayesian framework, a variation of what can be found in general treatments of probabilistic inference\cite{pythonbayes}\cite{gelmanbayes}\cite{blais}. The terminology and variable names will be reused in subsequent sections.

\subsection{Given Parameters}

Let \(X\) be the input space and \(Y\) be the output space. The goal is to predict a \(label\) \(y \in Y\) from a corresponding input \(x \in X\), or to learn the function \(f: X \to Y\).

The training dataset \(D \in (X \times Y)^N\) consists of \(N\) examples of input-output pairs \((x, y) \in (X \times Y) \). \(D\) may be generated by an oracle which produces an arbitrary number of examples, or it might be a small and limited collection. In any case, inference will be based off of just these \(N\) examples.

Instances are labelled with a subscript \((x_n, y_n)\), where \(n \in \{0, 1, 2,\ldots,N-1\}\), or \(n \in N\) in ordinal notation.

\subsection{The Hypothesis Space}

Let \(H\) be the \textit{hypothesis space} whose members \(h \in H\) each encode a potential solution to the prediction problem. We assume that one of these solutions is correct. This temporary assumption makes Bayesian inference possible.

Each \(h \in H\) is considered a \textit{hypothesis} for the correct function \(f\). They are also called \textit{predictors}, \textit{models}, and \textit{solutions}. We will use the term predictor to emphasize its ultimate purpose.

In some setups, the predictors \(h \in H\) are \textit{directly predicting} \(y \in Y\) and are represented by a function from \(X\) to \(Y\). Here instead the predictors will return a \textit{probability distribution} over \(Y\), which means that \(h \in H\) is now \textit{probabilistically predicting} \(y \in Y\). While it is possible to generalize to all \textit{probability measures} over \(Y\), we keep several cases in mind.

\begin{enumerate}
	\item \(Y\) is finite. This is a \textit{classification} problem. Each predictor takes an input \(x \in X\) and returns a probability for each \(y \in Y\) that sums to one.
	\item \(Y\) is discrete but infinite. The predictor still assigns a number to each potential output, but now the infinite sum adds to one.
	\item \(Y\) is continuous. This is a \textit{regression} problem. The predictor returns a \textit{probability distribution function} (PDF) over \(Y\), which integrates to one.
\end{enumerate}

In each of these cases, the model assigns a number to each \(y \in Y\). With discrete \(Y\) it is a finite probability, and with continuous \(Y\), it is the value of the PDF. If these numbers are normalized, the discrete probabilities will add to 1 and the continous PDF will integrate to 1.

Because these values may not be normalized at first, we identify each predictor \(h\) with a \textit{relative probability function} \(f_h: X \times Y \rightarrow \mathbb{R}_{\geq 0}\) which assignes a non-negative real number to each input-output pair. Define the normalized probability function \(\hat{f}_h: X \times Y \rightarrow \mathbb{R}_{\geq 0}\) as follows for both discrete and continuous \(Y\).

\begin{align}
\label{eq:normalized_probability_model}
\hat{f}_h(x, y)=\frac{f_h(x, y)}{\sum_{y' \in Y} f_h(x,y')} &
\qquad\hat{f}_h(x, y)=\frac{f_h(x, y)}{\int_{y' \in Y} f_h(x,y')}
\end{align}

\subsection{Deriving the Prior, Likelihood, and Posterior}

The \textit{prior distribution} over \(H\) is a probability distribution representing the initial belief over which predictor is correct. Typically this will be an \textit{uninformative prior} which encodes our absense of knowledge of the problem. It is also common to incorporate \textit{Occam's Razor} which penalizes more complex predictors, or to use a prior that is mathematically convenient. We will use \(\mathbf{P}(h)\) as the \textit{prior probability} of predictor \(h \in H\). This may denote a discrete probability or a value in a PDF in the continuous case.

\(\mathbf{P}(h)\) does not need to be normalized. It is enough that it represents a relative probability function on \(h\) which preserves the ratio of the probabilities between two predictors. This includes the possibility of an \textit{improper probability distribution function}. For example, a PDF that assigns 1 to each real number if \(H=\mathbb{R}\) cannot be normalized, but is not excluded. More importantly, allowing the prior to be unnormalized means that we can use distributions whose integral or sum is difficult to compute.

\(\mathbf{P}(D|h)\) is the \textit{likelihood function}. It denotes the probability of recieving the entire training set \(D\) under a given predictor \(h\). We assume that the examples in the training set are \textit{independent and identically distributed} (IID). This means that the likelihood is equal to the product of the probabilities of recieving each label \(y_n\) independently.

\begin{equation}
\label{eq:likelihood_expansion}
\mathbf{P}(D|h)=\prod_{n \in N} \hat{f}_h(x_n,y_n)
\end{equation}

\(\mathbf{P}(h|D)\) is the \textit{posterior distribution} over \(H\). The posterior represents the probability of each predictor being correct \textbf{after} the data has been taken into account.

Bayes rule for discrete \(H\) finds the posterior probability of each \(h \in H\), and in the continuous case it produces PDF values.

\begin{align}
\mathbf{P}(h|D)=\frac{\mathbf{P}(D|h)\mathbf{P}(h)}{\sum_{h' \in H}\mathbf{P}(D|h')\mathbf{P}(h')} &
\qquad\mathbf{P}(h|D)=\frac{\mathbf{P}(D|h)\mathbf{P}(h)}{\int_{h' \in H}\mathbf{P}(D|h')\mathbf{P}(h')}
\end{align}

Most learning algorithms only require unnormalized values for \(\mathbf{P}(h|D)\). We rewrite the equality as a proportionality statement and remove the denominator. This form also allows the prior \(\mathbf{P}(h)\) to be unnormalized as well, and generalizes both the discrete and continuous case.

\begin{equation}
\label{eq:bayes}
\mathbf{P}(h|D)\propto\mathbf{P}(D|h)\mathbf{P}(h)
\end{equation}

Going forward, any terms on the right hand side of this proportionality that are constant with respect to \(h\) can be removed. Using equation~\eqref{eq:likelihood_expansion}, we rewrite the formula for the relative posterior distribution as

\begin{equation}
\label{eq:bayes_likelihood_expanded}
\mathbf{P}(h|D)\propto \left( \prod_{n \in N} \hat{f}_h(x_n,y_n) \right) \mathbf{P}(h).
\end{equation}

\subsection{Selecting and Sampling Predictors}

The final learning task is to either \textit{select} or \textit{sample} predictors that best explain the data. This process involves a search of \(H\) and is often identified as the search algorithm or learning algorithm.

The goal of selection is to identify a single optimal predictor. The most obvious example is the \textit{maximum a posteriori} (MAP) estimate which chooses the predictor with the highest posterior probability. The \textit{maximum likelihood estimate} (MLE) finds the predictor that assigns the highest likelihood to the dataset, and ignores priors altogether.

Under sampling, a predictor is randomly pulled from the posterior distribution or something approximating it. If several models are sampled we can obtain a wide variety of possible solutions that are still consistent with the data.

Many methods for selection and sampling have been developed. These techniques include hill climbing and gradient descent for selection, and Markov Chain Monte Carlo for sampling. A good example of the latter is the \textit{No U-Turn Sampler}\cite{gelman} popular in the PyMC3\cite{pymc3} probabilistic programming package for python. The author has also deployed the Newton Raphson method for gradient descent to calculate the MLE.\cite{sklar_dirichlet}

It is more convenient to work with a \textit{negative log-likelihood loss} function than the posterior distribution directly.\footnote{For a great treatment on loss functions and their various tradeoffs, see A Tutorial on Energy Based Learning by Lecun\cite{lecun}. The work also discusses strategies for dealing with predictors where normalization over \(Y\) is not practical thus avoiding \(\hat{f_h}\).} Negative-log likelihood turns products into sums, and produces values that are within a reasonable order of magnitude for computation. We get this by applying \(-\ln(\ldots)\) to the right hand side of equation~\eqref{eq:bayes_likelihood_expanded}.

\[L(h)=-\sum_{n \in N} \ln(\hat{f}_h(x_n,y_n))-\ln(\mathbf{P}(h))\]

Let each hypothesis comes with its own negative log-likelihood loss function \(l_h\) where \(\hat{f}_h(x_n,y_n)\propto e^{-l_h(x_n,y_n)}\) and the prior \(\mathbf{P}(h)\) can be reduced to a \textit{regularization function} \(\mathbf{r}(h)\) where \(\mathbf{P}(h)\propto e^{-\mathbf{r}(h)}\). The final form of the loss function is therefore

\[L(h)=\sum_{n \in N} l_h(x_n,y_n)+\mathbf{r}(h).\]

\section{The Sampling Problem}
\label{section:problem}

What if the training set \(D\) was derived by downsampling a larger dataset \(D^+\)? Formally, we say that \(D\) was generated from \(D^+\) with a \textit{sampling probability function} \(\mathbf{s}\).

We limit ourselves to samplers \(\mathbf{s}: X \times Y \rightarrow \left [ 0, 1\right ]\) that consider each datapoint \((x, y) \in D^+\) independently.

This type of sampling is optimal for parallel computation because the sampler has no state other than it's inputs \((x, y)\). It does exclude some common sampling types, covered in Section \ref{section:future_work}.

\textbf{When the sampling function is known, a posterior distribution can still be computed from \(D\) to learn which predictors are more likely to fit \(D^+\)}.

\section{The General Solution}
\label{section:solution}

Start with the unnormalized Bayes rule in equation~\eqref{eq:bayes}, but now consider that the posterior distribution and likelihood both depend on the sampling function \(\mathbf{s}\).

\[\mathbf{P}(h|D,\mathbf{s})\propto\mathbf{P}(D|h,\mathbf{s})\mathbf{P}(h)\]

Let \(\mathbf{P}(x_n \in D^+)\) represent the probability that any given input \(x_n\) will appear in the unbiased dataset \(D^+\). The allows us to break down the likelihood as follows.

\[\mathbf{P}(D|h,s)=\prod_{n \in N} \mathbf{P}(x_n,y_n|h,\mathbf{s})=\prod_{n \in N} \mathbf{P}(x_n \in D^+)\mathbf{P}(y_n|x_n,h,\mathbf{s})\propto\prod_{n \in N}\mathbf{P}(y_n|x_n,h,\mathbf{s})\]

The term \(\mathbf{P}(x_n \in D^+)\) is constant with respect to \(h\) and can therefore be dropped in the proportionality statement. We are left with calculating the expression \(\mathbf{P}(y_n|x_n,h,\mathbf{s})\). The following \textit{generative description} is a useful tool in understanding how \(y_n\) is produced when there is a sampling function.

\begin{enumerate}
	\item Consider as given an input \(x_n\), a predictor \(h\), and a sampling function \(\mathbf{s}\). The predictor \(h\) encodes a probability distribution over \(Y\) through the function \(f_h(x_n,y_n)\).
	\item \label{l} Sample from that probability distribution and make this a candidate for \(y_n\), called \(y_n^*\)
	\item Compute the sampling rate \(\mathbf{s}(x_n,y_n^*)\) and use that rate to probabilistically determine whether \(y_n^*\) is accepted.
    \begin{enumerate}
        \item If it is accepted, return \(y_n=y_n^*\).
        \item If it is not accepted, return to step \ref{l} to generated another candidate.
    \end{enumerate}
\end{enumerate}

We now use the generative description to produce a recursive equation for \(\mathbf{P}(y_n|x_n,h,\mathbf{s})\). Let \(y \in Y\) be the first candidate for \(y_n\). If \(y\) is accepted, this probability is equal to 1 if \(y_n = y\) and 0 otherwise, given by the indicator function \(\left [y_n = y\right ]\).

If \(y\) is not accepted, then the probability reverts to the original value of \(\mathbf{P}(y_n|x_n,h,\mathbf{s})\). Putting it together, the probabilty of ultimately accepting \(y_n\) comes to

\[P(y_n|cand=y,x_n,h,\mathbf{s})=\mathbf{s}(x_n,y)\left [y_n = y\right ] + (1-\mathbf{s}(x_n,y))P(y_n|x_n,h,\mathbf{s}).\]

If \(Y\) is discrete, the probability of selecting \(y\) as a candidate in the first place is \(\hat{f}_h(x_n, y)\). Use this to sum over the probabilities of selecting each possible candidate and setup a recursive equation.

\begin{equation}
\label{eq:bias_corrected_setup}
\mathbf{P}(y_n|x_n,h,\mathbf{s})=\sum_{y \in Y}\hat{f}_h(x_n,y)\big(\mathbf{s}(x_n,y)\left [y_n = y\right ] + (1-\mathbf{s}(x_n,y))P(y_n|x_n,h,\mathbf{s})\big)
\end{equation}

With algebraic manipuation documented in appendix \ref{appendix:solving}, we solve for \(\mathbf{P}(y_n|x_n,h,\mathbf{s})\), reduce \(\hat{f}\) to \(f\), and derive the formulas for both discrete and continuous\footnote{The continuous version of this argument requires more mathematical background but is completely analogous. It requires integrating over all candidates instead of taking the sum. Some care must be taken with the indicator function term for a rigorous argument, but ultimately it can be reduced in the same way.} \(Y\).

\begin{align}
\label{eq:bias_corrected_prob}
\mathbf{P}(y_n|x_n,h,\mathbf{s})=\frac{f_h(x_n,y_n)\mathbf{s}(s_n,y_n)}{\sum_{y \in Y}f_h(x_n,y)\mathbf{s}(x_n,y)} &
\qquad\mathbf{P}(y_n|x_n,h,\mathbf{s})=\frac{f_h(x_n,y_n)\mathbf{s}(s_n,y_n)}{\int_{y \in Y}f_h(x_n,y)\mathbf{s}(x_n,y)}
\end{align}

The feasibility of computing the sum or integral term depends on the structure of \(Y\), but it is at least easy when \(Y\) is finite and small enough to be enumerated by a machine.

\subsection{Formula for Negative Log-Likelihood}

Equation~\eqref{eq:bias_corrected_prob} provides all the tools needed to assign relative posterior probabilities to predictors. We derive a negative log likelihood loss function starting with the relative Bayes formula.

\[\mathbf{P}(h|D,\mathbf{s})\propto\prod_{n \in N} \left[\mathbf{P}(y_n|x_n,h,\mathbf{s})\right]\mathbf{P}(h)\]

Use equation~\eqref{eq:bias_corrected_prob} to get this in terms of \(f_h\):

\[\mathbf{P}(h|D,\mathbf{s})\propto\prod_{n \in N} \left[\frac{f_h(x_n,y_n)\mathbf{s}(x_n,y_n)}{\sum_{y \in Y}f_h(x_n,y)\mathbf{s}(x_n,y)}\right]\mathbf{P}(h)\]

Finally, derive a negative log likelihood loss function on \(h\):

\[L(h)= \sum_{n \in N} \left[l_h(x_n,y_n)-\ln\mathbf{s}(x_n,y_n)+\ln\sum_{y \in Y}f_h(x_n,y)\mathbf{s}(x_n,y) \right] +\mathbf{r}(h)\]

\section{Solution for Binary Logistic Regression}
\label{section:logistic}

\textit{Binary logistic regression} is a special case of the supervised learning problem in section \ref{framework} with the following additional properties:

\begin{enumerate}
	\item It is a \textit{binary classification} in that \(Y = \{0, 1\}\).
	\item The input space \(X\) is a list of real valued features. Let \(F\) denotes a finite set of features, \(X = \mathbb{R} ^{|F|}\).
	\item Each predictor \(h \in H\) is parameterized by \(h = (c, \mathbf{w})\) where \(c \in \mathbb{R}\) is the intercept and \(\mathbf{w} \in \mathbb{R}^{|F|}\) is an \(|F|\)-dimentional vector of weights corresponding to each input feature.
          \item Each hypothesis \((c, \mathbf{w}) \in H\) corresponds to the following probability distribution function:
          \[f_{c,\mathbf{w}}(x_n, y_n)=\frac{e^{y_n \cdot (c+\mathbf{w} \cdot x_n)}}{1+e^{c+\mathbf{w} \cdot x_n}}\]
\end{enumerate}

Use equation~\eqref{eq:bias_corrected_prob} to get

\[\mathbf{P}(y_n|x_n,h,\mathbf{s})=\frac{f_h(x_n,y_n)\mathbf{s}(s_n,y_n)}{\sum_{y \in Y}f_h(x_n,y)\mathbf{s}(x_n,y)}=\frac{e^{y_n \cdot (c+\mathbf{w} \cdot x_n)}\mathbf{s}(s_n,y_n)}{\mathbf{s}(x_n,0)+e^{c+\mathbf{w} \cdot x_n}\mathbf{s}(x_n,1)}.\]

These problems are often framed to focus on the probability of the \textit{target condition} \(y_n = 1\). This target condition is usually the rare event that triggered the decision to use biased sampling, and would correspond to the lion image in section \ref{section:lion} and the visit in section \ref{section:visit}. We can solve for it as follows:

\begin{align}
\mathbf{P}(y_n=1|x_n,h,\mathbf{s})=\frac{e^{c+\mathbf{w} \cdot x_n}\mathbf{s}(s_n,1)}{\mathbf{s}(x_n,0)+e^{c+\mathbf{w} \cdot x_n}\mathbf{s}(x_n,1)}=\frac{e^{c+\mathbf{w} \cdot x_n}}{\mathbf{s}_r(x_n)+e^{c+\mathbf{w} \cdot x_n}} &
\qquad \textrm{where} \quad \mathbf{s}_r(x_n)=\frac{\mathbf{s}(x_n, 0)}{\mathbf{s}(x_n, 1)}
\end{align}

Here, \(\mathbf{s}_r: X \to [0, \infty]\) is the true-to-false \textit{sample ratio} for each instance \(n\). This ratio is all that is needed to correct for sampling bias in any binary classification. Note that for \(\mathbf{s}_r(x_n)=1\), we are left with the original binary logistic regression formula.

For the prior, we can use a Gaussian distribution (aka L2, or ridge regression) with weight \(\lambda\) so:

\[\mathbf{r}(c,\mathbf{w})=\frac{1}{2}\lambda(\mathbf{w} \cdot \mathbf{w})\]

Put this together and drop some constant factors to derive a negative log-likelihood loss function.

\[L(h)=\sum_{n \in  N} \left (\ln\left (\mathbf{s}_r(x_n)+e^{c+\mathbf{w} \cdot x_n}\right ) -y_n \cdot (c+\mathbf{w} \cdot x_n) \right )+ \frac{1}{2}\lambda(\mathbf{w} \cdot \mathbf{w})\]

The derivative on a single weight \(\mathbf{w}_f\) or intercept \(c\) can be computed into the following simple form in order to perform gradient descent.

\begin{align}
\frac{\partial}{\partial \mathbf{w}_f }L(h)=\sum_{n \in  N} x_{n,f}\left(\frac{e^{c+\mathbf{w} \cdot x_n}}{\mathbf{s}_r(x_n)+e^{c+\mathbf{w} \cdot x_n}} -y_n  \right )+ \lambda \cdot w_f
\qquad \frac{\partial}{\partial c}L(h)=\sum_{n \in  N} \left(\frac{e^{c+\mathbf{w} \cdot x_n}}{\mathbf{s}_r(x_n)+e^{c+\mathbf{w} \cdot x_n}}  -y_n\right )
\end{align}

\section{Future Work}
\label{section:future_work}

\subsection{Simple Random Sampling}

\textit{Simple Random Sampling} (SRS) is where an exact number of datapoints are retained in the dataset. The point-wise sampling function \(\mathbf{s}\) can never guarantee a specified number of instances will be selected. Simple Random Sampling can furthermore be stratefied by label to account for rare events. Because the sampling process is no longer independent by instance, deriving the formula is far more complex task.

\subsection{Oversampling}

The sampling function assumes that each datapoint is either included in the dataset or excluded. This is known as \textit{undersampling}. We could allow for \textit{oversampling}, where some instances are included in \(D\) multiple times. \textit{Bootstrap sampling} methods rely on oversampling.

Now instead of \(\mathbf{s}: X \times Y \rightarrow \left [ 0, 1\right ]\), the sampling function \(s\) returns a probability distribution over all natural numbers. This could be any probability distribution, but in practice it is often the \textit{poisson distribution}. The poisson distribution with parameter \(\lambda\) places probability of multiplicity \(k\) at \(\frac{\lambda^k e^{-\lambda}}{k!}\).

\subsection{The Value of an Instance}

The ability to quantify and quickly estimate the expected value that an instance will provide if included in \(D\) would allow engineers to deploy samplers that eliminate unneccesary work.

Perhaps insights on sampling can be borrowed from the field of \textit{active learning}, where learners can directly choose inputs to query. Angluin\cite{angluin} provides foundational analysis on the efficiencies that can be achived with active strategies. Many such strategies have been deployed for statistical models, and for example Cohn et al.\cite{active} propose selecting input data to minimize learner variance.

Ultimately, we do not know in general how much a datapoint will change the posterior distribution before it is included, but estimation techniques could be developed. Engineers must also consider the specific goals and tradeoffs of a project to inform their choice of sampling strategy. 

\subsection{Ensemble Models}

If models produced from several sampling functions can be combined into an \textit{ensemble model}, they will likely perform better than in the singular approach. Because they can be run in parallel, and each part runs quickly with a small sample size, this provides real engineering benefits.

\begin{appendices}

\section{Solving for the General Formula}
\label{appendix:solving}

Here we start with equation~\eqref{eq:bias_corrected_setup} and go through the series of steps neccesary to derive its final form in equation ~\eqref{eq:bias_corrected_prob}. Equation~\eqref{eq:bias_corrected_setup} begins as

\[\mathbf{P}(y_n|x_n,h,\mathbf{s})=\sum_{y \in Y}\hat{f}_h(x_n,y)\big(\mathbf{s}(x_n,y)\left [y_n = y\right ] + (1-\mathbf{s}(x_n,y))P(y_n|x_n,h,\mathbf{s})\big).\]

Break out the summation to get

\[\mathbf{P}(y_n|x_n,h,\mathbf{s})=\sum_{y \in Y}\hat{f}_h(x_n,y)\mathbf{s}(x_n,y)\left [y_n = y\right ] +\sum_{y \in Y}\hat{f}_h(x_n,y)(1-\mathbf{s}(s_n,y))P(y_n|x_n,h,\mathbf{s}).\]

In the first sum, the only non-zero addend is \(y = y_n\). Therefore, we can replace \(y\) with \(y_n\) and remove the summation and indicator function. In the second sum, factor out \(P(y_n|x_n,h,\mathbf{s})\) which does not contain summation index \(y\).

\[\mathbf{P}(y_n|x_n,h,\mathbf{s})=\hat{f}_h(x_n,y_n)\mathbf{s}(x_n,y_n) +P(y_n|x_n,h,\mathbf{s})\sum_{y \in Y}\hat{f}_h(x_n,y)(1-\mathbf{s}(s_n,y))\]

Collect the term \(\mathbf{P}(y_n|x_n,h,\mathbf{s})\) and distribute \(\hat{f}_h(x_n,y)\) in the remaining summation.

\[\mathbf{P}(y_n|x_n,h,\mathbf{s})\left [ 1 - \sum_{y \in Y}\hat{f}_h(x_n,y)(1-\mathbf{s}(x_n,y)) \right ]=\hat{f}_h(x_n,y_n)\mathbf{s}(s_n,y_n) \]

\[\mathbf{P}(y_n|x_n,h,\mathbf{s})\left [ 1 - \sum_{y \in Y}\hat{f}_h(x_n,y)+\sum_{y \in Y}\hat{f}_h(x_n,y)\mathbf{s}(x_n,y) \right ]=\hat{f}_h(x_n,y_n)\mathbf{s}(s_n,y_n) \]

Fom equation~\eqref{eq:normalized_probability_model} we know that for all possible inputs \(x\), \(\sum_{y \in Y} \hat{f}_h(x, y) = 1\). We can simplify as follows:

\[\mathbf{P}(y_n|x_n,h,\mathbf{s})\left [ 1 - 1+\sum_{y \in Y}\hat{f}_h(x_n,y)\mathbf{s}(s_n,y) \right ]=\hat{f}_h(x_n,y_n)\mathbf{s}(x_n,y_n) \]

\[\mathbf{P}(y_n|x_n,h,\mathbf{s})\left [\sum_{y \in Y}\hat{f}_h(x_n,y)\mathbf{s}(s_n,y) \right ]=\hat{f}_h(x_n,y_n)\mathbf{s}(x_n,y_n) \]

Also from equation~\eqref{eq:normalized_probability_model}, the \(\hat{f}_h\) terms are simply a constant factor of same terms with the unnormalized version \(f_h\). Because this factor appears on both sides of the equation, \(\hat{f}_h\) can be reduced to \(f\) through cancellation. Thus with one extra step of division, the final form can be given as ~\eqref{eq:bias_corrected_prob}.

\begin{equation}
\mathbf{P}(y_n|x_n,h,\mathbf{s})=\dfrac{f_h(x_n,y_n)\mathbf{s}(s_n,y_n)}{\sum_{y \in Y}f_h(x_n,y)\mathbf{s}(x_n,y)}
\tag{\ref{eq:bias_corrected_prob}}
\end{equation}

If the sampling was uniform where \(\mathbf{s}(x,y)=p\), equation~\eqref{eq:bias_corrected_prob} should reduce to the original predictor probability function \(\hat{f}_h(x_n,y_n)\) as a sanity check.

\[\mathbf{P}(y_n|x_n,h,\mathbf{s})=\frac{f_h(x_n,y_n)\mathbf{s}(x_n,y_n)}{\sum_{y \in Y}f_h(x_n,y)\mathbf{s}(x_n,y)}=\frac{f_h(x_n,y_n)p}{\sum_{y \in Y}f_h(x_n,y)p} =\hat{f}_h(x_n,y_n)\]

\end{appendices}

%\subsubsection*{References}

\begin{thebibliography}{20}

\bibitem{angluin}Angluin, D. (1988). Queries and concept learning. Machine learning, 2(4), 319-342.
\bibitem{blais}Blais, B. S. (2014). Statistical Inference for Everyone (sie).
\bibitem{active}Cohn, D. A., Ghahramani, Z., \& Jordan, M. I. (1996). Active learning with statistical models. Journal of artificial intelligence research, 4, 129-145.
\bibitem{gelmanbayes}Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., \& Rubin, B. D. (2013). Bayesian Data Analysis. 3rd edition.
\bibitem{gelman}Hoffman, M. D., \& Gelman, A. (2014). The No-U-Turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo. J. Mach. Learn. Res., 15(1), 1593-1623.
\bibitem{king}King, G., \& Zeng, L. (2001). Logistic regression in rare events data. Political analysis, 9(2), 137-163.
\bibitem{lecun}LeCun, Y., Chopra, S., Hadsell, R., Ranzato, M., \& Huang, F. (2006). A tutorial on energy-based learning. Predicting structured data, 1(0).
\bibitem{weightedlogistic}Maalouf, M., \& Siddiqi, M. (2014). Weighted logistic regression for large-scale imbalanced and rare events data. Knowledge-Based Systems, 59, 142-148.
\bibitem{rareevents}Maalouf, M., \& Trafalis, T. B. (2011). Rare events and imbalanced datasets: an overview. International Journal of Data Mining, Modelling and Management, 3(4), 375-388.
\bibitem{pythonbayes}Martin, O. (2016). Bayesian analysis with python. Packt Publishing Ltd.
\bibitem{pymc3}Salvatier, J., Wiecki, T. V., \& Fonnesbeck, C. (2016). Probabilistic programming in Python using PyMC3. PeerJ Computer Science, 2, e55.
\bibitem{sklar_dirichlet}Sklar, M. (2014). Fast MLE computation for the Dirichlet multinomial. arXiv preprint arXiv:1405.0099.
\bibitem{visitprediction}Sklar, M., Stewart, R., Li, R., Bakula, A., \& Spears, E. (2020). U.S. Patent Application No. 16/405,481. [ Section 0037 ]

\end{thebibliography}
\end{document}
